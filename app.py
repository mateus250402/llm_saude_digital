from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
import os
import warnings

warnings.filterwarnings("ignore")

os.environ["GOOGLE_API_KEY"] = "AIzaSyCNEiMNGmgfh_cL8Xr89xnc7PsMfeIvsEc"

# Carregar PDF
loader = PyPDFLoader("teste_grande.pdf")
docs = loader.load() 

# Dividir em chunks menores para melhorar a busca
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
docs = splitter.split_documents(docs)
if not docs:
    raise ValueError("Nenhum documento foi carregado ou o PDF est√° vazio.")

# Criar embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Banco vetorial
db = FAISS.from_documents(docs, embeddings)

# Retriever que pega mais trechos relevantes
retriever = db.as_retriever(search_kwargs={"k": 1000})

# Prompt flex√≠vel para qualquer pergunta
prompt_template = """
Responda √† pergunta apenas com base no conte√∫do fornecido do documento.
Voc√™ √© um assistente da √°rea da sa√∫de.
Se a informa√ß√£o aparecer em forma de lista no documento, copie todos os itens.
Se n√£o houver resposta no documento, diga: "N√£o sei com base no documento."
Sempre exibir o nome do documento e a(s) pagina(s) do PDF de onde a informa√ß√£o foi retirada, essa p√°gina √© a do leitor de PDF, desconsidere a pagina√ß√£o do documento.
Formatar a resposta em markdown com t√≠tulo e a pergunta feita pelo usu√°rio."

Contexto (trecho do documento): {context}

Pergunta: {input}
"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "input"],
)

# Conectar LLM Gemini
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

# Cadeia moderna de QA
stuff_chain = create_stuff_documents_chain(
    llm=llm,
    prompt=prompt,
    document_variable_name="context"
)

qa_chain = create_retrieval_chain(
    retriever=retriever,
    combine_docs_chain=stuff_chain
)

# Hist√≥rico
historico = []

# Loop
while True:
    query = input("üë§ Digite sua pergunta: ")

    if query.lower() in ["sair", "exit", "quit"]:
        print("Encerrando...")
        break

    resposta = qa_chain.invoke({"input": query})

    print("\nü§ñ Agente de IA:")
    print(resposta['answer'], "\n")

    historico.append({"pergunta": query, "resposta": resposta['answer']})